{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "5XpkzpoxIG_R",
        "outputId": "b39c08b7-240b-4cc7-e8b1-53a54f50a0d5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n",
        "!mkdir -p 11868\n",
        "%cd /content/drive/MyDrive/11868\n",
        "!git clone https://github.com/llmsystem/llmsys_code_examples.git\n",
        "%cd /content/drive/MyDrive/11868/llmsys_code_examples/tensor_demo/miniTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH9OYx-XJNGQ"
      },
      "source": [
        "## MiniTorch Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X19up_KxJG3y",
        "outputId": "3ada8ad7-c715-4d50-8f3d-a22f6ac67ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting colorama==0.4.3 (from -r requirements.txt (line 1))\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "Collecting hypothesis==6.54 (from -r requirements.txt (line 2))\n",
            "  Downloading hypothesis-6.54.0-py3-none-any.whl (389 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy==0.971 (from -r requirements.txt (line 3))\n",
            "  Downloading mypy-0.971-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba==0.58.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.58.1)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.23.5)\n",
            "Collecting pre-commit==2.20.0 (from -r requirements.txt (line 6))\n",
            "  Downloading pre_commit-2.20.0-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest==7.1.2 (from -r requirements.txt (line 7))\n",
            "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest-env (from -r requirements.txt (line 8))\n",
            "  Downloading pytest_env-1.1.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting pytest-runner==5.2 (from -r requirements.txt (line 9))\n",
            "  Downloading pytest_runner-5.2-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.5.0)\n",
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2024.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis==6.54->-r requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis==6.54->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from hypothesis==6.54->-r requirements.txt (line 2)) (1.2.0)\n",
            "Collecting mypy-extensions>=0.4.3 (from mypy==0.971->-r requirements.txt (line 3))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy==0.971->-r requirements.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.58.1->-r requirements.txt (line 4)) (0.41.1)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit==2.20.0->-r requirements.txt (line 6))\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit==2.20.0->-r requirements.txt (line 6))\n",
            "  Downloading identify-2.5.33-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit==2.20.0->-r requirements.txt (line 6))\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit==2.20.0->-r requirements.txt (line 6)) (6.0.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pre-commit==2.20.0->-r requirements.txt (line 6)) (0.10.2)\n",
            "Collecting virtualenv>=20.0.8 (from pre-commit==2.20.0->-r requirements.txt (line 6))\n",
            "  Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (1.3.0)\n",
            "Collecting py>=1.8.2 (from pytest==7.1.2->-r requirements.txt (line 7))\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pytest-env to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytest-env (from -r requirements.txt (line 8))\n",
            "  Downloading pytest_env-1.1.2-py3-none-any.whl (6.2 kB)\n",
            "  Downloading pytest_env-1.1.1-py3-none-any.whl (6.2 kB)\n",
            "  Downloading pytest_env-1.1.0-py3-none-any.whl (6.2 kB)\n",
            "  Downloading pytest_env-1.0.1-py3-none-any.whl (5.3 kB)\n",
            "  Downloading pytest_env-1.0.0-py3-none-any.whl (5.3 kB)\n",
            "  Downloading pytest_env-0.8.2-py3-none-any.whl (5.3 kB)\n",
            "  Downloading pytest_env-0.8.1-py3-none-any.whl (5.2 kB)\n",
            "INFO: pip is looking at multiple versions of pytest-env to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading pytest_env-0.8.0-py3-none-any.whl (5.2 kB)\n",
            "  Downloading pytest_env-0.7.0-py3-none-any.whl (4.9 kB)\n",
            "  Downloading pytest-env-0.6.2.tar.gz (1.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.10/dist-packages (from pycuda->-r requirements.txt (line 11)) (2023.1.1)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda->-r requirements.txt (line 11)) (1.4.4)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.10/dist-packages (from pycuda->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit==2.20.0->-r requirements.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda->-r requirements.txt (line 11)) (4.1.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.0.8->pre-commit==2.20.0->-r requirements.txt (line 6))\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.0.8->pre-commit==2.20.0->-r requirements.txt (line 6)) (3.13.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda->-r requirements.txt (line 11)) (2.1.3)\n",
            "Building wheels for collected packages: pytest-env\n",
            "  Building wheel for pytest-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytest-env: filename=pytest_env-0.6.2-py3-none-any.whl size=2344 sha256=9db37a9f9ffb5acf6c659ee365c606cc1a551f5d137730c261999ca5899ff2e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/dc/5a/862133b6d4688cfc367733985063f559f3b37ba99e62098458\n",
            "Successfully built pytest-env\n",
            "Installing collected packages: distlib, virtualenv, pytest-runner, py, nodeenv, mypy-extensions, identify, hypothesis, colorama, cfgv, pytest, pre-commit, mypy, pytest-env\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.4\n",
            "    Uninstalling pytest-7.4.4:\n",
            "      Successfully uninstalled pytest-7.4.4\n",
            "Successfully installed cfgv-3.4.0 colorama-0.4.3 distlib-0.3.8 hypothesis-6.54.0 identify-2.5.33 mypy-0.971 mypy-extensions-1.0.0 nodeenv-1.8.0 pre-commit-2.20.0 py-1.11.0 pytest-7.1.2 pytest-env-0.6.2 pytest-runner-5.2 virtualenv-20.25.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB5PVywaJgpS",
        "outputId": "d08940b9-ab90-40f8-cc70-340bc5451f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets==2.4.0 (from -r requirements.extra.txt (line 1))\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting embeddings==0.0.8 (from -r requirements.extra.txt (line 2))\n",
            "  Downloading embeddings-0.0.8-py3-none-any.whl (12 kB)\n",
            "Collecting networkx==2.4 (from -r requirements.extra.txt (line 3))\n",
            "  Downloading networkx-2.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plotly==4.14.3 (from -r requirements.extra.txt (line 4))\n",
            "  Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydot==1.4.1 (from -r requirements.extra.txt (line 5))\n",
            "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-mnist (from -r requirements.extra.txt (line 6))\n",
            "  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting streamlit==1.12.0 (from -r requirements.extra.txt (line 7))\n",
            "  Downloading streamlit-1.12.0-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit-ace (from -r requirements.extra.txt (line 8))\n",
            "  Downloading streamlit_ace-0.1.1-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.extra.txt (line 9)) (2.1.0+cu121)\n",
            "Collecting watchdog==1.0.2 (from -r requirements.extra.txt (line 10))\n",
            "  Downloading watchdog-1.0.2-py3-none-manylinux2014_x86_64.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (10.0.1)\n",
            "Collecting dill<0.3.6 (from datasets==2.4.0->-r requirements.extra.txt (line 1))\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (3.4.1)\n",
            "Collecting multiprocess (from datasets==2.4.0->-r requirements.extra.txt (line 1))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->-r requirements.extra.txt (line 1)) (23.2)\n",
            "Collecting responses<0.19 (from datasets==2.4.0->-r requirements.extra.txt (line 1))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from networkx==2.4->-r requirements.extra.txt (line 3)) (4.4.2)\n",
            "Collecting retrying>=1.3.3 (from plotly==4.14.3->-r requirements.extra.txt (line 4))\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from plotly==4.14.3->-r requirements.extra.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot==1.4.1->-r requirements.extra.txt (line 5)) (3.1.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (4.2.2)\n",
            "Requirement already satisfied: blinker>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (1.4)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (5.3.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (7.0.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (3.20.3)\n",
            "Collecting pydeck>=0.1.dev5 (from streamlit==1.12.0->-r requirements.extra.txt (line 7))\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pympler>=0.9 (from streamlit==1.12.0->-r requirements.extra.txt (line 7))\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (13.7.0)\n",
            "Collecting semver (from streamlit==1.12.0->-r requirements.extra.txt (line 7))\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (0.10.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (6.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (4.5.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.12.0->-r requirements.extra.txt (line 7)) (5.2)\n",
            "Collecting validators>=0.2 (from streamlit==1.12.0->-r requirements.extra.txt (line 7))\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19 (from streamlit==1.12.0->-r requirements.extra.txt (line 7))\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.extra.txt (line 9)) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.extra.txt (line 9)) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.extra.txt (line 9)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.extra.txt (line 9)) (2.1.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=3.2.0->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=3.2.0->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=3.2.0->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.extra.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.extra.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.extra.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.extra.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.extra.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.extra.txt (line 1)) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19->streamlit==1.12.0->-r requirements.extra.txt (line 7))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.4.0->-r requirements.extra.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.4.0->-r requirements.extra.txt (line 1)) (2023.3.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.extra.txt (line 9)) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->-r requirements.extra.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->-r requirements.extra.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->-r requirements.extra.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->-r requirements.extra.txt (line 1)) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (2.16.1)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.4.0->-r requirements.extra.txt (line 1))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.extra.txt (line 9)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.12.0->-r requirements.extra.txt (line 7))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (0.16.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->streamlit==1.12.0->-r requirements.extra.txt (line 7)) (0.1.2)\n",
            "Installing collected packages: python-mnist, watchdog, validators, smmap, semver, retrying, pympler, pydot, networkx, dill, responses, pydeck, plotly, multiprocess, gitdb, embeddings, gitpython, datasets, streamlit, streamlit-ace\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 1.4.2\n",
            "    Uninstalling pydot-1.4.2:\n",
            "      Successfully uninstalled pydot-1.4.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2.1\n",
            "    Uninstalling networkx-3.2.1:\n",
            "      Successfully uninstalled networkx-3.2.1\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.15.0\n",
            "    Uninstalling plotly-5.15.0:\n",
            "      Successfully uninstalled plotly-5.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.4.0 dill-0.3.5.1 embeddings-0.0.8 gitdb-4.0.11 gitpython-3.1.41 multiprocess-0.70.13 networkx-2.4 plotly-4.14.3 pydeck-0.8.1b0 pydot-1.4.1 pympler-1.0.1 python-mnist-0.7 responses-0.18.0 retrying-1.3.4 semver-3.0.2 smmap-5.0.1 streamlit-1.12.0 streamlit-ace-0.1.1 validators-0.22.0 watchdog-1.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.extra.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIF9pXvKKLPF",
        "outputId": "ed8b676a-92a9-41a0-ac2d-cf2397183829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/MyDrive/11868/assignment1/miniTorch\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: minitorch\n",
            "  Running setup.py develop for minitorch\n",
            "Successfully installed minitorch-0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -Ue ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxA8MeBIhCI9"
      },
      "outputs": [],
      "source": [
        "!bash compile_cuda.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15_DC_j_KVWI"
      },
      "outputs": [],
      "source": [
        "import minitorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Go Through Minitorch\n",
        "\n",
        "This is a high level abstraction of some basic components in MiniTorch.\n",
        "\n",
        "<img src=\"./imgs/high_level_abstraction.png\"></img>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Parameter:\n",
        "    \"\"\"\n",
        "    A Parameter is a special container stored in a `Module`.\n",
        "\n",
        "    It is designed to hold a `Variable`, but we allow it to hold\n",
        "    any value for testing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x: Any, name: Optional[str] = None) -> None:\n",
        "        self.value = x\n",
        "        self.name = name\n",
        "        if hasattr(x, \"requires_grad_\"):\n",
        "            self.value.requires_grad_(True)\n",
        "            if self.name:\n",
        "                self.value.name = self.name\n",
        "    \n",
        "    def update(self, x: Any) -> None:\n",
        "        \"Update the parameter value.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Module:\n",
        "    \"\"\"\n",
        "    Modules form a tree that store parameters and other\n",
        "    submodules. They make up the basis of neural network stacks.\n",
        "\n",
        "    Attributes:\n",
        "        _modules : Storage of the child modules\n",
        "        _parameters : Storage of the module's parameters\n",
        "        training : Whether the module is in training mode or evaluation mode\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _modules: Dict[str, Module]\n",
        "    _parameters: Dict[str, Parameter]\n",
        "    training: bool\n",
        "\n",
        "    def parameters(self) -> Sequence[Parameter]:\n",
        "        \"Enumerate over all the parameters of this module and its descendents.\"\n",
        "        return [j for _, j in self.named_parameters()]\n",
        "    \n",
        "    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"Set the mode of this module and all descendent modules to `train`.\"\n",
        "        for m in self.modules():\n",
        "            m.train()\n",
        "        self.training = True\n",
        "\n",
        "    def eval(self) -> None:\n",
        "        \"Set the mode of this module and all descendent modules to `eval`.\"\n",
        "        for m in self.modules():\n",
        "            m.eval()\n",
        "        self.training = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization\n",
        "def RParam(*shape):\n",
        "    r = 0.1 * (minitorch.rand(shape, backend=BACKEND) - 0.5)\n",
        "    return minitorch.Parameter(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, parameters: Sequence[Parameter]):\n",
        "        self.parameters = parameters\n",
        "    \n",
        "    def zero_grad(self) -> None:\n",
        "        for p in self.parameters:\n",
        "            if p.value is None:\n",
        "                continue\n",
        "            if hasattr(p.value, \"derivative\"):\n",
        "                if p.value.derivative is not None:\n",
        "                    p.value.derivative = None\n",
        "            if hasattr(p.value, \"grad\"):\n",
        "                if p.value.grad is not None:\n",
        "                    p.value.grad = None\n",
        "\n",
        "    def _print(self) -> None:\n",
        "        for param in self.parameters:\n",
        "            if param.value is None:\n",
        "                continue\n",
        "            print(param.value.shape)\n",
        "            print(param.value.grad)\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, parameters: Sequence[Parameter], lr: float = 1.0):\n",
        "        super().__init__(parameters)\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self) -> None:\n",
        "        for p in self.parameters:\n",
        "            if p.value is None:\n",
        "                continue\n",
        "            elif hasattr(p.value, \"grad\"):\n",
        "                if p.value.grad is not None:\n",
        "                    p.update(p.value - self.lr * p.value.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Function:\n",
        "    @classmethod\n",
        "    def _backward(cls, ctx: Context, grad_out: Tensor) -> Tuple[Tensor, ...]:\n",
        "        return wrap_tuple(cls.backward(ctx, grad_out))  # type: ignore\n",
        "\n",
        "    @classmethod\n",
        "    def _forward(cls, ctx: Context, *inps: Tensor) -> Tensor:\n",
        "        return cls.forward(ctx, *inps)  # type: ignore\n",
        "\n",
        "    @classmethod\n",
        "    def apply(cls, *vals: Tensor) -> Tensor:\n",
        "        raw_vals = []\n",
        "        need_grad = False\n",
        "        for v in vals:\n",
        "            if v.requires_grad():\n",
        "                need_grad = True\n",
        "            raw_vals.append(v.detach())\n",
        "\n",
        "        # Create the context.\n",
        "        ctx = Context(not need_grad)\n",
        "\n",
        "        # Call forward with the variables.\n",
        "        c = cls._forward(ctx, *raw_vals)\n",
        "\n",
        "        # Create a new variable from the result with a new history.\n",
        "        back = None\n",
        "        if need_grad:\n",
        "            back = minitorch.History(cls, ctx, vals)\n",
        "        return minitorch.Tensor(c._tensor, back, backend=c.backend)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Add(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx: Context, t1: Tensor, t2: Tensor) -> Tensor:\n",
        "        return t1.f.add_zip(t1, t2)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        return grad_output, grad_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tensor:\n",
        "    \"\"\"\n",
        "    Tensor is a generalization of Scalar in that it is a Variable that\n",
        "    handles multidimensional arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    backend: TensorBackend\n",
        "    history: Optional[History]\n",
        "    grad: Optional[Tensor]\n",
        "    _tensor: TensorData\n",
        "    unique_id: int\n",
        "    name: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Storage and Operators\n",
        "\n",
        "This diagram shows several important components of `class Tensor`, including data storage `class TensorData`, backend `class TensorBackend`, etc. Backend relies on `class TensorOps` to execute the actual computation for different operators. Basic functions are implemented and abstracted by `class Function`.\n",
        "\n",
        "<img src=\"./imgs/data_storage_operators.png\"></img>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy.typing as npt\n",
        "from typing import Sequence\n",
        "from typing_extensions import TypeAlias\n",
        "\n",
        "Storage: TypeAlias = npt.NDArray[np.float64]\n",
        "OutIndex: TypeAlias = npt.NDArray[np.int32]\n",
        "Index: TypeAlias = npt.NDArray[np.int32]\n",
        "Shape: TypeAlias = npt.NDArray[np.int32]\n",
        "Strides: TypeAlias = npt.NDArray[np.int32]\n",
        "\n",
        "UserIndex: TypeAlias = Sequence[int]\n",
        "UserShape: TypeAlias = Sequence[int]\n",
        "UserStrides: TypeAlias = Sequence[int]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorData:\n",
        "    _storage: Storage\n",
        "    _strides: Strides\n",
        "    _shape: Shape\n",
        "    strides: UserStrides\n",
        "    shape: UserShape\n",
        "    dims: int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Indexing\n",
        "\n",
        "<img src=\"./imgs/strides.png\"></img>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = minitorch.tensor([1, 2, 3, 4, 5, 6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x._tensor.shape, x._tensor.strides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = minitorch.Tensor.make(\n",
        "    storage=x._tensor._storage, \n",
        "    shape=(2, 3), \n",
        "    strides=(3, 1),\n",
        "    backend=x.backend)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y._tensor._storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y._tensor._storage is x._tensor._storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z = x.view(3, 2, 1)\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z._tensor.shape, y._tensor.strides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z._tensor._storage is x._tensor._storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z_index = [1, 1, 0]\n",
        "pos = minitorch.index_to_position(z_index, z._tensor._strides)\n",
        "z[tuple(z_index)] == z._tensor._storage[pos], pos, z[tuple(z_index)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_index = [0, 0, 0]\n",
        "minitorch.to_index(3, z.shape, out_index)\n",
        "out_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shape = (2, 3)\n",
        "minitorch.strides_from_shape(shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shape = (3, 2, 1)\n",
        "minitorch.strides_from_shape(shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = z.permute(2, 1, 0)\n",
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p._tensor.shape, p._tensor.strides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p._tensor._storage is x._tensor._storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_index = [0, 0, 0]\n",
        "minitorch.to_index(-1, p.shape, p_index)\n",
        "p_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "minitorch.index_to_position(p_index, p._tensor._strides)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Backend & Operators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorOps:\n",
        "    @staticmethod\n",
        "    def map(fn: Callable[[float], float]) -> MapProto:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def zip(fn: Callable[[float, float], float]) -> Callable[[Tensor, Tensor], Tensor]:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def reduce(\n",
        "        fn: Callable[[float, float], float], start: float = 0.0\n",
        "    ) -> Callable[[Tensor, int], Tensor]:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def matrix_multiply(a: Tensor, b: Tensor) -> Tensor:\n",
        "        raise NotImplementedError(\"Not implemented in this assignment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorBackend:\n",
        "    def __init__(self, ops: Type[TensorOps]):\n",
        "        # Maps\n",
        "        self.neg_map = ops.map(operators.neg)\n",
        "        self.sigmoid_map = ops.map(operators.sigmoid)\n",
        "        self.relu_map = ops.map(operators.relu)\n",
        "        self.log_map = ops.map(operators.log)\n",
        "        self.exp_map = ops.map(operators.exp)\n",
        "        self.id_map = ops.map(operators.id)\n",
        "        self.id_cmap = ops.cmap(operators.id)\n",
        "        self.inv_map = ops.map(operators.inv)\n",
        "\n",
        "        # Zips\n",
        "        self.add_zip = ops.zip(operators.add)\n",
        "        self.mul_zip = ops.zip(operators.mul)\n",
        "        self.lt_zip = ops.zip(operators.lt)\n",
        "        self.eq_zip = ops.zip(operators.eq)\n",
        "        self.is_close_zip = ops.zip(operators.is_close)\n",
        "        self.relu_back_zip = ops.zip(operators.relu_back)\n",
        "        self.log_back_zip = ops.zip(operators.log_back)\n",
        "        self.inv_back_zip = ops.zip(operators.inv_back)\n",
        "\n",
        "        # Reduce\n",
        "        self.add_reduce = ops.reduce(operators.add, 0.0)\n",
        "        self.mul_reduce = ops.reduce(operators.mul, 1.0)\n",
        "\n",
        "        # Matrix Multiply\n",
        "        self.matrix_multiply = ops.matrix_multiply\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CudaKernelOps(TensorOps):\n",
        "    @staticmethod\n",
        "    def map(fn: Callable[[float], float]) -> MapProto:\n",
        "        ### Your Implementation ###\n",
        "\n",
        "    @staticmethod\n",
        "    def zip(fn: Callable[[float, float], float]) -> Callable[[Tensor, Tensor], Tensor]:\n",
        "        ### Your Implementation ###\n",
        "\n",
        "    @staticmethod\n",
        "    def reduce(\n",
        "        fn: Callable[[float, float], float], start: float = 0.0\n",
        "    ) -> Callable[[Tensor, int], Tensor]:\n",
        "        ### Your Implementation ###\n",
        "\n",
        "    @staticmethod\n",
        "    def matrix_multiply(a: Tensor, b: Tensor) -> Tensor:\n",
        "        ### Your Implementation ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In project/run_sentiment.py\n",
        "backend_name = \"CudaKernelOps\"\n",
        "\n",
        "if backend_name == \"CudaKernelOps\":\n",
        "    from minitorch.cuda_kernel_ops import CudaKernelOps\n",
        "    BACKEND = minitorch.TensorBackend(CudaKernelOps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connect to CUDA Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map(fn: Callable[[float], float]) -> MapProto:\n",
        "    \"See `tensor_ops.py`\"\n",
        "    fn_id = fn_map[fn]\n",
        "\n",
        "    def ret(a: Tensor, out: Optional[Tensor] = None) -> Tensor:\n",
        "        if out is None:\n",
        "            out = a.zeros(a.shape)\n",
        "\n",
        "        # Define the argument type for the tensorMap function\n",
        "        lib.tensorMap.argtypes = [\n",
        "            ctypes.POINTER(ctypes.c_double),  # out\n",
        "            ctypes.POINTER(ctypes.c_int),    # out_shape\n",
        "            ctypes.POINTER(ctypes.c_int),    # out_strides\n",
        "            ctypes.c_int,                    # out_size\n",
        "            ctypes.POINTER(ctypes.c_double),  # in_storage\n",
        "            ctypes.POINTER(ctypes.c_int),    # in_shape\n",
        "            ctypes.POINTER(ctypes.c_int),    # in_strides\n",
        "            ctypes.c_int,                    # shape_len\n",
        "            ctypes.c_int,                    # fn_id\n",
        "        ]\n",
        "\n",
        "        # Define the return type for the tensorMap function\n",
        "        lib.tensorMap.restype = None\n",
        "\n",
        "        # Convert the numpy arrays to gpuarrays that can be loaded to the gpu\n",
        "        out_array_gpu = gpuarray.to_gpu(out._tensor._storage)\n",
        "        out_shape_gpu = gpuarray.to_gpu(out._tensor._shape.astype(np.int32))\n",
        "        out_strides_gpu = gpuarray.to_gpu(out._tensor._strides.astype(np.int32))\n",
        "        in_array_gpu = gpuarray.to_gpu(a._tensor._storage)\n",
        "        in_shape_gpu = gpuarray.to_gpu(a._tensor._shape.astype(np.int32))\n",
        "        in_strides_gpu = gpuarray.to_gpu(a._tensor._strides.astype(np.int32))\n",
        "\n",
        "\n",
        "        # Call the function\n",
        "        lib.tensorMap(\n",
        "            ctypes.cast(out_array_gpu.ptr, ctypes.POINTER(ctypes.c_double)),\n",
        "            ctypes.cast(out_shape_gpu.ptr, ctypes.POINTER(ctypes.c_int)),\n",
        "            ctypes.cast(out_strides_gpu.ptr, ctypes.POINTER(ctypes.c_int)),\n",
        "            ctypes.c_int(out.size),\n",
        "            ctypes.cast(in_array_gpu.ptr, ctypes.POINTER(ctypes.c_double)),\n",
        "            ctypes.cast(in_shape_gpu.ptr, ctypes.POINTER(ctypes.c_int)),\n",
        "            ctypes.cast(in_strides_gpu.ptr, ctypes.POINTER(ctypes.c_int)),\n",
        "            ctypes.c_int(len(a.shape)),\n",
        "            ctypes.c_int(fn_id)\n",
        "        )\n",
        "            \n",
        "        # Copy the gpuarray back to the cpu\n",
        "        out._tensor._storage = out_array_gpu.get()\n",
        "\n",
        "        # Free the gpuarrays\n",
        "        out_array_gpu.gpudata.free()\n",
        "        out_shape_gpu.gpudata.free()\n",
        "        out_strides_gpu.gpudata.free()\n",
        "        in_array_gpu.gpudata.free()\n",
        "        in_shape_gpu.gpudata.free()\n",
        "        in_strides_gpu.gpudata.free()\n",
        "        return out\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "void tensorMap(\n",
        "    scalar_t* out, \n",
        "    int* out_shape, \n",
        "    int* out_strides, \n",
        "    int out_size, \n",
        "    scalar_t* in_storage, \n",
        "    int* in_shape, \n",
        "    int* in_strides,\n",
        "    int shape_size,\n",
        "    int fn_id\n",
        ") {\n",
        "    int threadsPerBlock = BASE_THREAD_NUM;\n",
        "    int blocksPerGrid = (out_size + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    mapKernel<<<blocksPerGrid, threadsPerBlock>>>(out, out_shape, out_strides, out_size, in_storage, in_shape, in_strides, shape_size, fn_id);\n",
        "    cudaDeviceSynchronize();\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IYJbDGLrIK93",
        "gH9OYx-XJNGQ",
        "mWRAVR1kQSFS",
        "QE0eesQ04d-W",
        "xSo94QvzCFbn",
        "7sejR8-MTPmM",
        "RdCXpyn3j9e-"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
