{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: merging \"e\" and \"r\"\n",
      "step 2: merging \"s\" and \"</w>\"\n",
      "step 3: merging \"e\" and \"</w>\"\n",
      "step 4: merging \"e\" and \"n\"\n",
      "step 5: merging \"d\" and \"</w>\"\n",
      "step 6: merging \"h\" and \"er\"\n",
      "step 7: merging \"en\" and \"t\"\n",
      "step 8: merging \"e\" and \"d</w>\"\n",
      "step 9: merging \",\" and \"</w>\"\n",
      "step 10: merging \"her\" and \"</w>\"\n",
      "step 11: merging \"n\" and \"</w>\"\n",
      "step 12: merging \"p\" and \"a\"\n",
      "step 13: merging \"pa\" and \"r\"\n",
      "step 14: merging \"par\" and \"ent\"\n",
      "step 15: merging \"en\" and \"</w>\"\n",
      "step 16: merging \"h\" and \"e</w>\"\n",
      "step 17: merging \"a\" and \"s</w>\"\n",
      "step 18: merging \"s\" and \"e\"\n",
      "step 19: merging \"e\" and \"a\"\n",
      "step 20: merging \"i\" and \"t\"\n",
      "The bpe tokens are: \n",
      ",: 0\n",
      "H: 1\n",
      "</w>: 2\n",
      "]: 3\n",
      "': 4\n",
      "v: 5\n",
      "s: 6\n",
      "y: 7\n",
      "m: 8\n",
      "a: 9\n",
      "w: 10\n",
      "c: 11\n",
      "r: 12\n",
      "f: 13\n",
      "h: 14\n",
      "i: 15\n",
      "T: 16\n",
      "D: 17\n",
      "W: 18\n",
      "g: 19\n",
      "t: 20\n",
      "n: 21\n",
      "O: 22\n",
      "B: 23\n",
      "k: 24\n",
      "l: 25\n",
      "p: 26\n",
      "b: 27\n",
      "[: 28\n",
      "o: 29\n",
      "6: 30\n",
      "u: 31\n",
      "e: 32\n",
      "d: 33\n",
      "er: 34\n",
      "s</w>: 35\n",
      "e</w>: 36\n",
      "en: 37\n",
      "d</w>: 38\n",
      "her: 39\n",
      "ent: 40\n",
      "ed</w>: 41\n",
      ",</w>: 42\n",
      "her</w>: 43\n",
      "n</w>: 44\n",
      "pa: 45\n",
      "par: 46\n",
      "parent: 47\n",
      "en</w>: 48\n",
      "he</w>: 49\n",
      "as</w>: 50\n",
      "se: 51\n",
      "ea: 52\n",
      "it: 53\n",
      "[23, 34, 8, 9, 21, 4, 35, 47, 35, 33, 15, 5, 29, 12, 11, 41, 10, 14, 48, 49, 10, 50, 51, 5, 48, 16, 39, 52, 13, 20, 34, 42, 49, 6, 26, 25, 53, 2, 20, 15, 8, 36, 27, 32, 20, 10, 32, 48, 52, 11, 14, 2, 47, 4, 35, 14, 29, 31, 51, 14, 29, 25, 38, 31, 21, 20, 15, 25, 2, 49, 40, 34, 41, 11, 29, 25, 25, 32, 19, 36, 28, 30, 3, 2, 1, 15, 35, 13, 9, 20, 43, 12, 32, 25, 29, 11, 9, 20, 41, 20, 29, 2, 17, 9, 25, 25, 50, 13, 29, 12, 2, 9, 2, 26, 29, 6, 53, 15, 29, 44, 50, 9, 2, 25, 29, 27, 27, 7, 15, 6, 20, 2, 29, 44, 27, 32, 14, 9, 25, 13, 2, 29, 13, 2, 13, 29, 29, 33, 51, 12, 5, 15, 11, 36, 27, 31, 6, 15, 21, 32, 6, 51, 6, 42, 10, 14, 15, 25, 36, 14, 15, 35, 8, 29, 20, 43, 8, 29, 5, 41, 27, 9, 11, 24, 2, 15, 44, 10, 53, 14, 2, 43, 47, 35, 15, 44, 18, 29, 29, 6, 20, 34, 42, 22, 14, 15, 29, 42, 9, 21, 38, 27, 32, 11, 9, 8, 36, 9, 2, 20, 52, 11, 43, 20, 39, 36]\n",
      "B er m a n ' s</w> parent s</w> d i v o r c ed</w> w h en</w> he</w> w as</w> se v en</w> T her ea f t er ,</w> he</w> s p l it </w> t i m e</w> b e t w e en</w> ea c h </w> parent ' s</w> h o u se h o l d</w> u n t i l </w> he</w> ent er ed</w> c o l l e g e</w> [ 6 ] </w> H i s</w> f a t her</w> r e l o c a t ed</w> t o </w> D a l l as</w> f o r </w> a </w> p o s it i o n</w> as</w> a </w> l o b b y i s t </w> o n</w> b e h a l f </w> o f </w> f o o d se r v i c e</w> b u s i n e s se s ,</w> w h i l e</w> h i s</w> m o t her</w> m o v ed</w> b a c k </w> i n</w> w it h </w> her</w> parent s</w> i n</w> W o o s t er ,</w> O h i o ,</w> a n d</w> b e c a m e</w> a </w> t ea c her</w> t her e</w>\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "from collections import defaultdict \n",
    "import string\n",
    "  \n",
    "def get_stats(vocab): \n",
    "    \"\"\" \n",
    "    Given a vocabulary (dictionary mapping words to frequency counts), returns a  \n",
    "    dictionary of tuples representing the frequency count of pairs of characters  \n",
    "    in the vocabulary. \n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int) \n",
    "    for word, freq in vocab.items(): \n",
    "        chars = word.split() # split the word by any white space\n",
    "        for i in range(len(chars)-1): \n",
    "            pairs[chars[i], chars[i+1]] += freq \n",
    "    return pairs \n",
    "  \n",
    "def merge_vocab(token_pair, v_in): \n",
    "    \"\"\" \n",
    "    Given a pair of characters and a vocabulary, returns a new vocabulary with the  \n",
    "    pair of characters merged together wherever they appear. \n",
    "    \"\"\"\n",
    "    v_out = defaultdict(int)  \n",
    "    bigram = re.escape(' '.join(token_pair)) \n",
    "    new_token = ''.join(token_pair)\n",
    "    # search for every occurance of bigram (token pairs with a space), \n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "    for word in v_in:\n",
    "        # replace the bigram (with space), with the new merged token (the concanated pair)\n",
    "        w_out = p.sub(new_token, word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "  \n",
    "def get_init_vocab(data): \n",
    "    \"\"\" \n",
    "    Given a list of strings, returns a dictionary of words mapping to their frequency  \n",
    "    count in the data. \n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    tokens = set()\n",
    "    tokens.add('</w>')\n",
    "    for line in data: \n",
    "        for word in line.split(): \n",
    "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "            tokens.update(list(word))\n",
    "    return vocab, tokens \n",
    "  \n",
    "def byte_pair_encoding(data, n): \n",
    "    \"\"\" \n",
    "    Given a list of strings and an integer n, returns a list of n merged pairs \n",
    "    of characters found in the vocabulary of the input data. \n",
    "    \"\"\"\n",
    "    vocab, init_tokens = get_init_vocab(data)\n",
    "    tokens = list(init_tokens)\n",
    "    for i in range(n): \n",
    "        pairs = get_stats(vocab) \n",
    "        best_pair = max(pairs, key=pairs.get) \n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        tokens.append(''.join(best_pair))\n",
    "        print('step {}: merging \\\"{}\\\" and \\\"{}\\\"'.format(i+1, best_pair[0], best_pair[1]))\n",
    "    return tokens\n",
    "\n",
    "def tokenize(data, token_dict):\n",
    "    \"\"\"split the data into a tokens and map into index\"\"\"\n",
    "    encoded_ids = []\n",
    "    for line in data: \n",
    "        for word in line.split():\n",
    "            word = word + '</w>'\n",
    "            last_idx = 0\n",
    "            idx = len(word)\n",
    "            while idx > last_idx:\n",
    "                whole_word = word[last_idx:idx]\n",
    "                if whole_word in token_dict:\n",
    "                    encoded_ids.append(token_dict[whole_word])\n",
    "                    last_idx = idx\n",
    "                    idx = len(word)\n",
    "                else:\n",
    "                    idx = idx - 1\n",
    "    return encoded_ids\n",
    "  \n",
    "# Example usage: \n",
    "corpus = '''Berman's parents divorced when he was seven. \n",
    "Thereafter, he split time between each parent's household until he entered college.[6] \n",
    "His father relocated to Dallas for a position as a lobbyist on behalf of foodservice businesses, \n",
    "while his mother moved back in with her parents in Wooster, Ohio, and became a teacher there'''\n",
    "data = corpus.split('.') \n",
    "  \n",
    "n = 20 # number of merge operations\n",
    "bpe_vocab = byte_pair_encoding(data, n) \n",
    "\n",
    "\n",
    "bpe_dict = dict([(tk, id) for id, tk in enumerate(bpe_vocab)])\n",
    "id_to_token = dict([(tid, tk) for tk, tid in bpe_dict.items()]  )\n",
    "\n",
    "token_ids = tokenize(data, bpe_dict)\n",
    "\n",
    "print(\"The bpe tokens are: \")\n",
    "for tk, tid in bpe_dict.items():\n",
    "    print(\"{}: {}\".format(tk, tid))\n",
    "\n",
    "print(token_ids)\n",
    "print(' '.join(id_to_token[tid] for tid in token_ids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
